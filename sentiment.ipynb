{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4683d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72d695ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e60ee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'IMDB Dataset.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdce059",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['review'].values\n",
    "y = df['sentiment'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,stratify=y)\n",
    "#print(x_train[0])\n",
    "#s = ''.join(x_train[0])\n",
    "#print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ccda05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2113eb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03dcedfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6197b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b8df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textCleaning(s):\n",
    "    s = re.sub('[^A-Za-z]+', '', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b34008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(textCleaning('PhdPassword@24'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232ec2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(x_train, y_train, x_test, y_test):\n",
    "    vocab = []\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for sent in x_train:\n",
    "        temp = ''\n",
    "        temp = ''.join(sent)\n",
    "        for word in temp.split():\n",
    "            word = word.lower()\n",
    "            word = textCleaning(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                #print(word)\n",
    "                vocab.append(word)\n",
    "                \n",
    "    corpus = Counter(vocab)\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)\n",
    "    #print(corpus)\n",
    "    \n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    x_final_train, x_final_test = [], []\n",
    "    \n",
    "    for sent in x_train:\n",
    "        temp = ''\n",
    "        temp = ''.join(sent)\n",
    "        x_final_train.append([onehot_dict[textCleaning(word.lower())] for word in temp.split() \n",
    "                             if textCleaning(word) in onehot_dict.keys()])\n",
    "    for sent in x_test:\n",
    "        temp = ''\n",
    "        temp = ''.join(sent)\n",
    "        x_final_test.append([onehot_dict[textCleaning(word.lower())] for word in temp.split() \n",
    "                             if textCleaning(word) in onehot_dict.keys()])\n",
    "        \n",
    "    y_final_train = [1 if label == 'positive' else 0 for label in y_train]\n",
    "    y_final_test = [1 if label == 'positive' else 0 for label in y_test]\n",
    "    \n",
    "    return x_final_train, x_final_test, y_final_train, y_final_test, onehot_dict\n",
    "    #return x_final_train\n",
    "    \n",
    "    #print(onehot_dict)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aecd507",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, onehot_dict = preProcessing(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73681a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cb8d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(x_train))\n",
    "x_train = np.array(x_train, dtype = object)\n",
    "x_test = np.array(x_test, dtype = object)\n",
    "y_train = np.array(y_train, dtype = np.int32)\n",
    "y_test = np.array(y_test, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25a5f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f768b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(data, seqLen):\n",
    "    features = np.zeros((len(data), seqLen),dtype=int)\n",
    "    for i, rev in enumerate(data):\n",
    "        if len(rev) != 0:\n",
    "            features[i, -len(rev):] = np.array(rev)[:seqLen]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7737c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = padding_(x_train,500)\n",
    "x_test_pad = padding_(x_test,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d287e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(x_train_pad[1])\n",
    "#print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3d6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter\n",
    "inputDimension = 300 # number of feature in input as we use glove 300 that converts every word in a 300 dimension feature vector that's why here inputdimension is 300\n",
    "hiddenSize = 64\n",
    "numLayer = 2 # how many RNN layers will be stacked togather to form the network\n",
    "bidirectional = False # True -> bidirectional RNN, LSTM, GRU\n",
    "batchFirst = True # decide the input, output shape\n",
    "outputSize = 2 # num of class\n",
    "direction = 1 # 1 when bidirectional is False otherwise it will be 2\n",
    "batchSize = 50\n",
    "seqLen = 500 # lenght of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8bf7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train_pad)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "\n",
    "x_test_tensor = torch.from_numpy(x_test_pad)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "997f1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c31bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37500])\n"
     ]
    }
   ],
   "source": [
    "print(y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d8bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batchSize)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fd95a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 500])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = next(dataiter)\n",
    "\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "#print('Sample input: \\n', sample_x)\n",
    "#print('Sample output: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c3f7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 500])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "print(sample_x.shape)\n",
    "print(sample_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04e9fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "384f4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, inputDimension, hiddenSize, numLayer, batchFirst, outputSize):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = inputDimension\n",
    "        self.hidden_size = hiddenSize\n",
    "        self.num_layer = numLayer\n",
    "        self.batch_first = batchFirst\n",
    "        self.outputSize = outputSize\n",
    "        #Embedding layer: weights of the embedding layer has been initialized by pre-train glove model rather then random values\n",
    "        self.embedding = nn.Embedding.from_pretrained(glove.vectors)\n",
    "        #RNN structure\n",
    "        self.rnn = torch.nn.RNN(input_size=inputDimension,\n",
    "                                hidden_size=hiddenSize,\n",
    "                                num_layers=numLayer,\n",
    "                                bidirectional=False,\n",
    "                                batch_first=batchFirst)\n",
    "        #dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        #fully connected layer\n",
    "        self.linear = torch.nn.Linear(hiddenSize, outputSize)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input shape -> batch size, sequence length\n",
    "        # hidden shape -> direction*num of layer, batch size, hidden size\n",
    "        h = torch.zeros(1 * numLayer, 50, hiddenSize)\n",
    "        embd = self.embedding(input)\n",
    "        # embd (input data) shape -> batch size, sequence length, input dimension (number of features in input)\n",
    "        # embedding layer takes input in shape (batch size, sequence length) converts the input shape as \n",
    "        # (batch size, sequence length, input dimension(number of features in input))\n",
    "        # print(embd.shape)\n",
    "       \n",
    "        output, hidden_ = self.rnn(embd, h)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output[:, -1, :])  # Get the output from the last time step\n",
    "        output = self.sig(output)\n",
    "        \n",
    "        return output, hidden_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "404c559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(400000, 300)\n",
      "  (rnn): RNN(300, 64, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = RNN(inputDimension, hiddenSize, numLayer, batchFirst, outputSize)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "607b6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "307cac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "<generator object Module.parameters at 0x0000021E734D9620>\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9635b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 0.6740, Training Accuracy: 57.72%\n",
      "Epoch 2\n",
      "Training Loss: 0.6667, Training Accuracy: 59.59%\n",
      "Epoch 3\n",
      "Training Loss: 0.6531, Training Accuracy: 62.45%\n"
     ]
    }
   ],
   "source": [
    "clip = 5\n",
    "epochs = 3\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "epoch_tr_loss, epoch_vl_loss = [], []\n",
    "epoch_tr_acc, epoch_vl_acc = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    \n",
    "    #h = torch.zeros(1 * numLayer, batchSize, hiddenSize)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs, _ = model(inputs)\n",
    "        # model output shape is [batch_size, num_classes]\n",
    "        # Convert the labels to LongTensor if they are not already\n",
    "        labels = labels.long()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_tr_loss.append(train_loss / len(train_loader))\n",
    "    epoch_tr_acc.append(train_correct / train_total)\n",
    "    \n",
    "    print(f'Training Loss: {epoch_tr_loss[-1]:.4f}, Training Accuracy: {epoch_tr_acc[-1]*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8292511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
